{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the word\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    \n",
    "    # print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(''.join(itos[i] for i in context), '===>', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "      \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From 5 words, we get 32 examples of context and label.\n",
    "# Each input to the neural net is the 3 integers and the \n",
    "# label is an integer Y\n",
    "# \n",
    "# Our goal is to build a neural net that takes X as input and predicts Y\n",
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 27 characters and we will embed them in a two-dimensional space\n",
    "# So, the lookup table will have 27 rows and 2 columns all consisting of\n",
    "# random numbers to start out.\n",
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6271, -1.3715],\n",
       "        [ 0.8982,  0.3304],\n",
       "        [ 1.8028,  0.8174],\n",
       "        [-0.1556,  1.6917],\n",
       "        [ 0.9456, -0.0075],\n",
       "        [ 0.8036, -0.0089],\n",
       "        [-0.2084,  1.1448],\n",
       "        [-0.5263, -0.0444],\n",
       "        [-0.5498, -0.5942],\n",
       "        [-0.6559, -0.5901],\n",
       "        [-0.0295, -0.4438],\n",
       "        [-0.0543, -0.5368],\n",
       "        [ 0.2836,  0.4046],\n",
       "        [-0.3199, -0.2108],\n",
       "        [ 0.1400,  0.4148],\n",
       "        [-1.3695,  1.3905],\n",
       "        [-0.3979,  0.4653],\n",
       "        [-1.7865,  0.5325],\n",
       "        [ 0.4113,  1.0469],\n",
       "        [-0.3758, -1.0029],\n",
       "        [ 0.3997,  0.4650],\n",
       "        [ 0.0697, -2.2284],\n",
       "        [ 0.9086,  1.1530],\n",
       "        [ 0.4446, -0.1869],\n",
       "        [-0.3659, -0.7266],\n",
       "        [-0.3019,  2.1032],\n",
       "        [-0.9161,  1.9215]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we embed all of the integers in the input X \n",
    "# using the lookup table C, we will see how we could \n",
    "# embed a single integer, say 5\n",
    "# One way is to take C[5] which gives a two-dimensional vector in C\n",
    "# In last lecture, we did something identical, but appears different.\n",
    "# i.e., we took these integers and one-hot encoded them. \n",
    "# F.one_hot(torch.tensor(5), num_classes=27).float(). \n",
    "# If we multiply this vector with C, we get an identical vector as output.\n",
    "# i.e., matrix multiplication\n",
    "#\n",
    "# In summary, we can interpret this embedding of the integer  \n",
    "# as either: 1) Integer indexing into lookup table C or \n",
    "# equivalently 2) as the first layer of a bigger neural net. This layer\n",
    "# has neurons that have no non-linearity and its weight matrix is C. \n",
    "# We are then encoding integers using One-Hot and feeding those integers\n",
    "# into the neural net. See the figure in the Language Modeling paper. \n",
    "#\n",
    "# We will index as it is much faster. Embedding a single integer like 5 \n",
    "# is easy - it is C[5]. But, how do we simultaneously embed all of the\n",
    "# 32 x 3 integers in the array X. pytorch indexing allows you to do this\n",
    "# fairly easily. \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8036, -0.0089],\n",
       "        [-0.2084,  1.1448],\n",
       "        [-0.5263, -0.0444]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch indexing using lists - example\n",
    "C[[5, 6, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8036, -0.0089],\n",
       "        [-0.2084,  1.1448],\n",
       "        [-0.5263, -0.0444],\n",
       "        [-0.5263, -0.0444],\n",
       "        [-0.5263, -0.0444]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you could also index with a tensor of integers - example\n",
    "C[torch.tensor([5, 6, 7, 7, 7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you could also index with a multidensional tensors of integers\n",
    "# So, C[X] is our embedding\n",
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hidden layer, we have:\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but, emb.shape is 32 x 3 x 2 and W1.shape is 6 x 100 which cannot be multiplied.\n",
    "# so emb @ W1 * b1 won't work\n",
    "# torch.cat can help with this - we concatenate\n",
    "# the three 32 x 2 matrices along the 2nd dimension (2nd parameter is 1)\n",
    "# to achieve this.\n",
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.unbind(emb, 1) is equivalent to the above and can\n",
    "# be used to apply in the general case as follows:\n",
    "# Below is very inefficient as each torch object creates new memory\n",
    "torch.cat(torch.unbind(emb, 1), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6271, -1.3715, -0.6271, -1.3715, -0.6271, -1.3715],\n",
       "        [-0.6271, -1.3715, -0.6271, -1.3715,  0.8036, -0.0089],\n",
       "        [-0.6271, -1.3715,  0.8036, -0.0089, -0.3199, -0.2108],\n",
       "        [ 0.8036, -0.0089, -0.3199, -0.2108, -0.3199, -0.2108],\n",
       "        [-0.3199, -0.2108, -0.3199, -0.2108,  0.8982,  0.3304],\n",
       "        [-0.6271, -1.3715, -0.6271, -1.3715, -0.6271, -1.3715],\n",
       "        [-0.6271, -1.3715, -0.6271, -1.3715, -1.3695,  1.3905],\n",
       "        [-0.6271, -1.3715, -1.3695,  1.3905,  0.2836,  0.4046],\n",
       "        [-1.3695,  1.3905,  0.2836,  0.4046, -0.6559, -0.5901],\n",
       "        [ 0.2836,  0.4046, -0.6559, -0.5901,  0.9086,  1.1530],\n",
       "        [-0.6559, -0.5901,  0.9086,  1.1530, -0.6559, -0.5901],\n",
       "        [ 0.9086,  1.1530, -0.6559, -0.5901,  0.8982,  0.3304],\n",
       "        [-0.6271, -1.3715, -0.6271, -1.3715, -0.6271, -1.3715],\n",
       "        [-0.6271, -1.3715, -0.6271, -1.3715,  0.8982,  0.3304],\n",
       "        [-0.6271, -1.3715,  0.8982,  0.3304,  0.9086,  1.1530],\n",
       "        [ 0.8982,  0.3304,  0.9086,  1.1530,  0.8982,  0.3304],\n",
       "        [-0.6271, -1.3715, -0.6271, -1.3715, -0.6271, -1.3715],\n",
       "        [-0.6271, -1.3715, -0.6271, -1.3715, -0.6559, -0.5901],\n",
       "        [-0.6271, -1.3715, -0.6559, -0.5901, -0.3758, -1.0029],\n",
       "        [-0.6559, -0.5901, -0.3758, -1.0029,  0.8982,  0.3304],\n",
       "        [-0.3758, -1.0029,  0.8982,  0.3304,  1.8028,  0.8174],\n",
       "        [ 0.8982,  0.3304,  1.8028,  0.8174,  0.8036, -0.0089],\n",
       "        [ 1.8028,  0.8174,  0.8036, -0.0089,  0.2836,  0.4046],\n",
       "        [ 0.8036, -0.0089,  0.2836,  0.4046,  0.2836,  0.4046],\n",
       "        [ 0.2836,  0.4046,  0.2836,  0.4046,  0.8982,  0.3304],\n",
       "        [-0.6271, -1.3715, -0.6271, -1.3715, -0.6271, -1.3715],\n",
       "        [-0.6271, -1.3715, -0.6271, -1.3715, -0.3758, -1.0029],\n",
       "        [-0.6271, -1.3715, -0.3758, -1.0029, -1.3695,  1.3905],\n",
       "        [-0.3758, -1.0029, -1.3695,  1.3905, -0.3979,  0.4653],\n",
       "        [-1.3695,  1.3905, -0.3979,  0.4653, -0.5498, -0.5942],\n",
       "        [-0.3979,  0.4653, -0.5498, -0.5942, -0.6559, -0.5901],\n",
       "        [-0.5498, -0.5942, -0.6559, -0.5901,  0.8982,  0.3304]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is an even better (performance) way to do this.\n",
    "# torch.view is an efficient way to rearrange dimensions\n",
    "# this is true because the way it is stored is as a one-dimensional\n",
    "# vector as seen in torch.storage - storage offsets, strides, etc. \n",
    "# so, we can simply ask pytorch to rearrange 32 x 3 x 2 as 32 x 6:\n",
    "emb.view(32, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6) == torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layer:\n",
    "# so, to do the multiplication for the hidden state, we can:\n",
    "# the first parameter -1 to emb.view below asks view to determine the dimension.\n",
    "# you could also use emb.shape[0] instead of -1 or hardcoding (32).\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9769,  1.0000,  0.7873,  ..., -0.2524,  0.9414, -0.9398],\n",
       "        [-0.5230,  0.1705, -0.8801,  ..., -0.9470,  0.9907, -0.1819],\n",
       "        [-0.0243,  0.9080, -0.9941,  ...,  0.9288, -0.2151,  0.9489],\n",
       "        ...,\n",
       "        [ 0.2173,  0.8704,  1.0000,  ..., -0.9972,  0.9782, -0.9999],\n",
       "        [-0.4947,  0.9639,  0.9990,  ..., -0.9428,  0.9489, -0.9979],\n",
       "        [ 0.3459, -0.9285,  0.0641,  ..., -0.9874,  0.9753, -0.7731]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layer\n",
    "# output dimension is 27 for each of the 26 alphabets or . that could come next\n",
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.8406)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilities assigned to the correct character in the sequence\n",
    "# that the neural network has assigned (randomly) is:\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************** REFACTORING CODE DONE SO FAR *************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: RESET THE OPTIMIZATION BY RUNNING THIS CELL\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: \n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.645933151245117\n",
      "13.093705177307129\n",
      "12.57966136932373\n",
      "12.09949779510498\n",
      "11.653555870056152\n",
      "11.243552207946777\n",
      "10.868578910827637\n",
      "10.525022506713867\n",
      "10.211129188537598\n",
      "9.924636840820312\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # forward pass\n",
    "    emb = C[X] # (~200K, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (~200K, 100)\n",
    "    logits = h @ W2 + b2 # (~200K, 27)\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdims=True)\n",
    "    # loss = -prob[torch.arange(32), Y].log().mean()\n",
    "    # pytorch can get the loss using the cross_entropy function much\n",
    "    # more efficiently than the three lines above as follows:\n",
    "    # Efficiency comes from two factors: a) it avoids creating the \n",
    "    # 3 intermediate tensors, and b) backward pass is much simpler.\n",
    "    # Also the exp operations can end up being nan for large numbers. This\n",
    "    # issue is side-stepped by cross_entropy by adding an offset equal to \n",
    "    # the maximum value in the vector\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3800690174102783\n"
     ]
    }
   ],
   "source": [
    "# the above loop takes a while on each iteration as we\n",
    "# process (forwarding & backwarding) more than 200k examples \n",
    "# with the full dataset\n",
    "# Instead, we can make it run faster by using mini-batches\n",
    "# of the data during iteration. i.e., we randomly select\n",
    "# a portion of the data to form the mini-batch. The \n",
    "# forward/backward/update is only done on that mini-batch.\n",
    "# we use torch.randint(0, X.shape[0], (32,)) to generate 32 numbers\n",
    "# between 0 and ~200K for a batch size of 32 instead of ~200K previously.\n",
    "# Using this reduced the quality of our gradient, but in practice\n",
    "# it turns out to be fine - it is better to have an approximate gradient\n",
    "# and do more steps than evaluating exact gradient and take fewer steps.\n",
    "for _ in range(1000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6001, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss for entire dataset\n",
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre\n",
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.29787278175354\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: \n",
    "# Determining a reasonable learning rate - \n",
    "# basically you could change the parameter lr \n",
    "# manually and set it when the loss that comes out \n",
    "# are stable (monotonically decreasing)\n",
    "# We use lre as follows:\n",
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    # print(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    # lr = lrs[i]\n",
    "    lr = 0.1\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    lri.append(lr)\n",
    "    lossi.append(loss.item())\n",
    "    \n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dc8b66b700>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above indicates around 0.1 is a good learning rate\n",
    "# after several iterations, you could decay the learning rate\n",
    "# by 10x --> 0.01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182580, 3]) torch.Size([182580])\n",
      "torch.Size([22767, 3]) torch.Size([22767])\n",
      "torch.Size([22799, 3]) torch.Size([22799])\n"
     ]
    }
   ],
   "source": [
    "# splitting data - training, dev (validation), test\n",
    "# 80% : 10% : 10% split is typical\n",
    "# training 80% used to optimize parameters of model as done above.\n",
    "# dev 10% is used to tune the hyperparameters (size of hidden layer, size of embedding, etc.)\n",
    "# test 10% is used to evaluate the performance of the model at the end.\n",
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.083493947982788\n"
     ]
    }
   ],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # print(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    # lr = lrs[i]\n",
    "    lr = 0.1\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    lri.append(lr)\n",
    "    lossi.append(loss.item())\n",
    "    \n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4572, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss on the dev set\n",
    "emb = C[Xtr]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4603, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss on the dev set\n",
    "emb = C[Xdev]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the loss on the training and dev are roughly equal, \n",
    "# we are not overfitting. So, we could increase the size of\n",
    "# the neural net. Easiest way is to increase the dimensions of\n",
    "# the hidden layer W1, b1 (from 100 to 300 say). Parameters will\n",
    "# go up significantly as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAAHSCAYAAAAuWvi9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3zU5Z33//cnM5MQEzB4IMRApSJgq5GiiNRTh0WrZNsVW/u79e5tUXuXn2y7t7bd7tqF3d4+7ge/7t73r7vWXYs/2lKx7W56UCxroa5lGw+1HESBAFYOnhKOHgiYEJI5XL8/JhNymEkmyZy+M6/n48GDZL7fmbkuSPLO9b2u7+cy55wAAEB+K8l1AwAAwNAIbAAAPIDABgDAAwhsAAA8gMAGAMADCGwAADzAn+sGDOacc85xU6ZMSfn89vZ2VVRUZK5BeYy+0/diQ9/peyHaunXru865cxMdy+vAnjJlil566aWUz29sbFQwGMxcg/IYfQ/muhk5Qd+DuW5GTtD3YK6bkTFm9layY1wSBwDAAwhsAAA8gMAGAMADRh3YZjbZzH5nZq+a2S4zuzfBOWZmD5nZPjPbYWaXjfZ9AQAoJulYdBaW9HXn3MtmNlbSVjN7xjm3u9c5CyRN6/5zpaQV3X8DAIAUjHqE7Zw75Jx7ufvjDyS9Kqm232k3S3rMxWyUVGVmNaN9bwAAikVa57DNbIqkWZI29TtUK6m51+ctGhjqAAAgCUvXfthmVinpWUnLnXNP9Dv2a0nfds690P35Bkl/5ZzbmuB1FktaLEnV1dWXNzQ0pNyGtrY2VVZWjrwTHkbf6Xuxoe/0vRDNmzdvq3NudqJjaSmcYmYBSY9L+mn/sO7WImlyr88nSTqY6LWccyslrZSk2bNnu+HcIF/oN9QPhr4Hc92MnKDvwVw3IyfoezDXzciJdKwSN0k/lPSqc+4fk5y2VtIXuleLz5V03Dl3aLTvDQBAsUjHCPtqSXdIajKzbd2P/Y2kD0mSc+4RSesk1UvaJ+mkpLvS8L4AgAIXjkR1MhRRRalfvhLLdXNyatSB3T0vPei/ootNlH95tO8FACh8neGI1jUd0orG/dp7tE3+ElM46jR9QqWWXBRSZziiMr8v183Murze/AMAUFy2NbfqzlWbFYpE1d4VkSSFIrHF0a8dadOB6oiuXL5Bq++eo5mTq3LZ1KyjNCkAIC9sb27V7Ss3qrUj1BPW/UWdU2tHSLet3Kjtza1ZbmFuEdgAgJzrDEe0aNVmdYQSB3V/HaHY+Z3h1M4vBFwSBwDk3LqmQwpFon0e+4s/uVALP1arg8c7dKy9S00HTkite3qOhyJRrW86rIWziqMOFyNsAEDOrWjc3+cyeF3tmVpwyUTVP/S87vnxVtVNGjhf3d4V0YrGfdlsZk4xwgYA5FQk6rT3aFufx66YMl7P7D6iznBUnZI2vHok4XP3HG1TJOqK4pYvRtgAgJxq7wrL3y9wYzW5huYvMbV3hTPRrLxDYAMAcqqi1K9wtO++FlvefF/zP1KtMn+Jzij1ad5FExI+Nxx1qigtjovFxdFLAEDe8pWYpk2o1J4jpy+L72g5rt++ekTr7r1WB451qKnluD44FdK4fs+dPqGyKC6HS4ywAQB5YElwqipK+1YvW/nc65r/nWe1+Mcv6YJzK9R04Hif4xWlPi0JXjjs9wpHojpxKqRIND27VWYLI2wAQM7V19XogbW7JZ1eKf7tz9Rp2oRKlfl9evzlFu06eEI3nX36OQFfiRbUTUzp9Qcrd3pPcKrq62ryvtwpgQ0AyLkyv0+r756j21Zu7Cmecm/DtqTnlwdi56cSskOVO122ZqceWLs778udckkcAJAXZk6uUsPiuaoqDwy4PB5XYqaq8oAaFs9NKVxTKXfa3hXxRLlTAhsAkDdmTq7SpqXztfyWOs2orpSZFPCZzKQZ1WNVO75cm5bOTymsC63cKZfEAQB5pczv08JZtVo4q1aRqFN7V7hnP+zGxsaU55oTlTudNL5cq++ao23NrfroeeP0xrvt+trPt+lUKHZePpc7ZYQNAMhbvhLTuDGBEd261b/cadzUCZX6181va8F3n1dbZ1h3zJ3Scyyfy50S2ACAgpOo3GncgdYObX3rmCRpzSsHdMWU8X2Ox8ud5hsCGwBQcBKVO41zrm8Y94/mfC13SmADAApOonKncZPGn6HLPhRbtPZnM8/Tljff73M8X8udEtgAgIITL3eayN4jH+izl03S+nuvVdUZAf1k41t9judrudP8+xUCAIA0WBKcqmVrdg5YeBZ10tIndyZ8zkjLnWYDI2wAQEGqr6tRwDe8mBtOudNsI7ABAAUpXu60PHD6vu2WYx268cHnEp4/nHKnuUBgAwAKVirlTitKfcMqd5orzGEDAApavNzp+qbDWtG4T3v67NY1VkuCU7WgbmLejqzjCGwAQMEbrNypV3BJHEBWhCNRnTgVyssKUiguoyl3mkuMsAFkTGc4onVNh7Sicb/29rkMWal7glNVX1eT95chgXxBYAPIiG3Nrbpz1WaFItGe+2BDkdjo+rUjbVq2ZqceWLtbq++ek9cLfYB8wSVxAGm3vblVt6/cqNaOUJ+iFbseuLHn4/auiFo7Qrpt5UZtb27NRTMBTyGwAaRVZziiRas2qyM0cFvDRDpCsfM7w6mdDxQrAhtAWq1rOqRQJDqs54QiUa1vOpyhFgGFgcAGkFYrGvcPqN08lPauiFY07stQi4DCQGADSJtI1Gnv0bYRPXfP0TZu+QIGQWADSJv2rrD8I7y31V9iau8Kp7lFQOEgsAGkTUWpX+ERjpLDUaeKUu40BZIhsAGkja/ENG1C5YieO31CpecqTwHZRGADSKslwalJd0W6+FtPJ3y8otSnJcELM9kswPMIbABpVV9Xo4BveD9aAr4SLaibmKEWAYWBwAaQVmV+n1bfPUflgdRqhJcHYudTUxwYHIENIO1mTq5Sw+K5qioPJL08XlHqU1V5QA2L51JLHEgBSzIBZMTMyVXatHS+1jcd1orGfdrTZ7eusVoSnKoFdRMZWQMpIrABZEyZ36eFs2q1cFatIlGn9q6wKkr9rAYHRoDABpAVvhLTuDGBXDcD8CzmsAEA8AACGwAADyCwAQDwAAIbAAAPILABAPAAAhsAAA8gsAEA8AACGwAADyCwAQDwAAIbAAAPILABAPAAAhsAAA8gsAEA8AACGwAADyCwAQDwAAIbAAAPILABAPAAAhsAAA8gsAEA8AACGwAADyCwAQDwAAIbAAAPILABAPAAAhsAAA9IS2Cb2SozO2pmO5McD5rZcTPb1v3n79LxvgAAFAt/ml7nUUn/IumxQc553jn3qTS9HwAARSUtI2zn3HOS3k/HawEAgIGyOYf9cTPbbmbrzeziLL4vAACeZ8659LyQ2RRJTznnLklwbJykqHOuzczqJX3XOTctyesslrRYkqqrqy9vaGhIuQ1tbW2qrKwcQeu9j77T92JD3+l7IZo3b95W59zsRMeyEtgJzn1T0mzn3LuDnTd79mz30ksvpdyGxsZGBYPBlM8vJPQ9mOtm5AR9D+a6GTlB34O5bkbGmFnSwM7KJXEzm2hm1v3xnO73fS8b7w0AQCFIyypxM/s3SUFJ55hZi6RvSQpIknPuEUm3SlpiZmFJHZJuc+ka2gMAUATSEtjOuduHOP4vit32BQAARoBKZwAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgA0E84EtWJUyFFoi7XTQF6+HPdAADIB53hiNY1HdKKxv3ae7RN/hJTOOo0fUKl7glOVX1djcr8vlw3E0WMwAZQ9LY1t+rOVZsVikTV3hWRJIUisdH1a0fatGzNTj2wdrdW3z1HMydX5bKpKGJcEgdQ1LY3t+r2lRvV2hHqCev+2rsiau0I6baVG7W9uTXLLQRiCGwARaszHNGiVZvVEUoc1P11hGLnd4ZTOx9IJwIbQNFa13RIoUi0z2OTxpfr6fuu6/n8S9deoPuun9bzeSgS1fqmw1lrIxBHYAMoWisa9ye9DJ5Me1dEKxr3ZahFQHIENoCiFIk67T3aNqLn7jnaxi1fyDoCG0BRau8Ky19iAx4PR5x6P1wWGPhj0l9iau8KZ7J5wAAENoCiVFHqVzjBKPndtk6dXVmmqjMCKvWVaP5FEwacE446VZRyVyyyi684AEXJV2KaNqFSe470vSwejjo9tGGvnvzzq9V87KT2vzPwsvn0CZXyJRidA5lEYAMoWkuCU7Vszc4BC88effFNPfrimwmfU1Hq05LghVloHdAXl8QBFK36uhoFfMP7MRjwlWhB3cQMtQhIjsAGULTK/D6tvnuOygOp1QgvD8TOp6Y4coHABlDUZk6uUsPiuaoqD6iiNHEQV5T6VFUeUMPiudQSR84whw2g6M2cXKVNS+drfdNhrWjcpz19dusaqyXBqVpQN5GRNXKKwAYAxS6PL5xVq4WzahWJOrV3hVVR6mc1OPJGWi6Jm9kqMztqZjuTHDcze8jM9pnZDjO7LB3vCwCZ4CsxjRsTIKyRV9I1h/2opJsGOb5A0rTuP4slrUjT+wIAUBTSEtjOueckvT/IKTdLeszFbJRUZWY16XhvAACKQbZWiddKau71eUv3YwAAIAXmXHp2nDGzKZKecs5dkuDYryV92zn3QvfnGyT9lXNua4JzFyt22VzV1dWXNzQ0pNyGtrY2VVZWjqj9Xkff6Xuxoe/0vRDNmzdvq3NudqJj2Vol3iJpcq/PJ0k6mOhE59xKSSslafbs2S4YDKb8Jo2NjRrO+YWEvgdz3YycoO/BXDcjJ+h7MNfNyIlsXRJfK+kL3avF50o67pw7lKX3BgDA89Iywjazf5MUlHSOmbVI+pakgCQ55x6RtE5SvaR9kk5Kuisd7wsAQLFIS2A7524f4riT9OV0vBcAAMWIWuIAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeEBaAtvMbjKz18xsn5ndn+B40MyOm9m27j9/l473BQCgWPhH+wJm5pP0sKQbJLVI2mJma51zu/ud+rxz7lOjfT8AAIpROkbYcyTtc8697pzrktQg6eY0vC4AAOhmzrnRvYDZrZJucs799+7P75B0pXPuK73OCUp6XLER+EFJf+mc25Xk9RZLWixJ1dXVlzc0NKTclra2NlVWVo6wJ95G3+l7saHv9L0QzZs3b6tzbnaiY6O+JC7JEjzW/7eAlyWd75xrM7N6SU9KmpboxZxzKyWtlKTZs2e7YDCYckMaGxs1nPMLCX0P5roZOUHfg7luRk7Q92Cum5ET6bgk3iJpcq/PJyk2iu7hnDvhnGvr/nidpICZnZOG9wYAoCikI7C3SJpmZh82s1JJt0la2/sEM5toZtb98Zzu930vDe8NAEBRGPUlcedc2My+IulpST5Jq5xzu8zsnu7jj0i6VdISMwtL6pB0mxvt5DkAAEUkHXPY8cvc6/o99kivj/9F0r+k470AAChGVDoDAMADCGwAADyAwAYAwAMIbAAAPIDABgDAAwhsAAA8gMAGAMADCGwAADyAwAYAwAMIbAAAPIDABgDAAwhsAAA8gMAG8lA4EtWJUyFFomxqByAmLbt1ARi9znBE65oOaUXjfu092iZ/iSkcdZo+oVL3BKeqvq5GZX5frpsJIEcIbCAPbGtu1Z2rNisUiaq9KyJJCkVio+vXjrRp2ZqdemDtbq2+e45mTq7KZVMB5AiXxIEc297cqttXblRrR6gnrPtr74qotSOk21Zu1Pbm1iy3EEA+ILCBHOoMR7Ro1WZ1hBIHdX8dodj5neHUzgdQOLgkDuTQuqZDCkWifR5b+LFa3Xn1FJX6TNuaW7XsyZ3qvfYsFIlqfdNhLZxVm+XWAsglRthADq1o3N/nMvjUcyv1qZk1unXFi6p/6AVFohoQzO1dEa1o3JftpgLIMUbYQI5Eok57j7b1eezqC89WXe2ZWvuVqyVJZQGf3mvvHPDcPUfbuOULKDIENpAj7V1h+UusZzW4JJmZHt/aov/99GuDPtdfYmrvCme6iQDyCJfEgRypKPUr3G+U/Pt972pBXY3OriiVJJ1ZHlBtVfmA54ajThWl/L4NFBO+44Ec8ZWYpk2o1J4jpy+L7zvapu/8x2v68RfnyMwUjjj93a926kBrR5/nTp9QKV+JZbvJAHKIwAZyaElwqpat2dln4dlTOw7pqR2Hkj6notSnJcELs9E8AHmES+JADtXX1SjgG963YcBXogV1EzPUIgD5isAGcqjM79Pqu+eoPJBajfDyQOx8aooDxYfABnJs5uQqNSyeq6rygCpKEwdxRalPVeUBNSyeSy1xoEgxhw3kgZmTq7Rp6XytbzqsFY37tKfPbl1jtSQ4VQvqJjKyBooYgQ3kiTK/Twtn1WrhrFpFok7tXWFVlPoztho8HInqZCiS0fcAkD4ENjKGQBg5X4lp3JhA2l+XPbcB7yKwkVYEQv5iz23A21h0hrTZ1tyqK5dv0LI1O7XnSJuciwWCc6cD4crlG9jPOQfYcxvwPgIbaUEg5C/23AYKA4GNURssEB5fctWAxwiE7Eq057YkffGaD+vp+67T0/ddp7uvntLnWHzPbQD5g8DGqCULBEn67IoXEz5OIGRP/z23JemS2nH63OxJWvjw73XL936v2+Z8SBefN67nOHtuA/mHwMaoJQqEuF0P3JjwcQIhOxLtuS1JV0w5S0/vOqKOUEQnuyL6zc7DumLKWX3OYc9tIL8Q2BiVZIGQCgIh8+J7bveXyk127LkN5BcCG6OSLBBSQSBkXqI9tyVp0xvv65MfrdaYQInKAz7dePFEbXnz/T7nsOc2kF/4bsSoJAuEVBAImZdoz21J2nXwhH65tUW/+vI1kqSfbXlbuw6e6HMOe24D+YWflhiVZIGQCgIhOxLtuS1JP3zhDf3whTcSPoc9t4H8wyVxjNqS4NSku0wlQyBkD3tuA4WBwMaoDRYIF3/r6YSPEwjZw57bQGEgsDFqBEL+Y89twPsIbKQFgZD/4ntuL7+lTjOqK2UmBXwmM2lG9Vgtv6VOm5bO5/8GyFMsOkPaxANhfdNhrWjcpz19dusaqyXBqVpQN5GRdQ5le89tAOlDYCOtCATvyNSe2wAyg8BGxhAIAJA+zGEDAOABBDYAAB5AYAMA4AEENgAAHkBgAwDgAQR2loUjUZ04FWIfaADAsHBbVxZ0hiNa13RIKxr3a2+fYiKVuic4VfV1NRQTAQAMisDOsG3Nrbpz1WaFItGe7Q1Dkdjo+rUjbVq2ZqceWLtbq++eQ0lIAEBSXBLPoO3Nrbp95Ua1doQG7EUc194VUWtHSLet3Kjtza1ZbiEAwCsI7AzpDEe0aNVmdYQSB3V/HaHY+Z3h1M4HABQXAjtD1jUdUigSHdZzQpGo1jcdzlCLAABeRmBnyIrG/X0ug3/thum66+opPZ//5Sdn6M6rpvR5TntXRCsa92WphQAALyGwMyASddp7tK3PYz9/qVmfvWySJMlM+vTMGj257cCA5+452sYtXwCAAVglngHtXWH5S6xnNbgktRzr0LGTXbr4vHE6p7JMuw6eUOvJ0IDn+ktM7V1hdrkCAPRBYGdARalf4QSj5J9tadatl0/SuZVl+vlLzQmfG446VZTy3wIA6ItL4hngKzFNm1A54PGndx3WddPP1aWTqvTcnncSPnf6hEr5SizTTQQAeAyBnSFLglNVUdq3elko4rRx/3v6ddNBJZqmrij1aUnwwiy1EADgJQR2htTX1Sjg6/vPaybN+lCVfrYl8eXwgK9EC+omZqN5QE5QSx8YOSZLM6TM79Pqu+fotpUb1RGK6MIJlVq16Ao9vfuw3nzv5IDzywOx86kpjkJDLX0gPQjsDJo5uUoNi+dq0arNOtTaoev+z+8GnFNR6lPAV0ItcRQkaukD6cMl8QybOblKm5bO1/Jb6jSjulJmUsBnMpNmVI/V8lvqtGnpfH5YoeBQSx9Ir7SMsM3sJknfleST9APn3N/3O27dx+slnZR0p3Pu5XS8txeU+X1aOKtWC2fVKhJ1au8Kq6LUz2pwFKyR1tLftHQ+l8eBJEYd2Gbmk/SwpBsktUjaYmZrnXO7e522QNK07j9XSlrR/XfR8ZUYRVFQ8JLV0l95x+WqObNcZYES/ej3b+jfNp9egBmvpb9wVm02mwp4RjpG2HMk7XPOvS5JZtYg6WZJvQP7ZkmPOeecpI1mVmVmNc65Q2l4fwB5pn8t/bhv/HKHjneEVOYv0dqvXKP1Ow/3VPyL19InsIHE0hHYtZJ636fUooGj50Tn1EoisIECk6iWftxdV0/RjRfHbl2sqRqjD59doVdOnp67jtfSZ7oIGCgdgZ3oO6v/TZapnBM70WyxpMWSVF1drcbGxpQb0tbWNqzzCwl9b8x1M3IiH/sedU5fr4sodkHttNoJZ2vupWfrV//5nMKRiG65/mrdMd3pT6rCPeeYmZ59tlElNnRg52Pfs4W+N+a6GTmRjsBukTS51+eTJB0cwTmSJOfcSkkrJWn27NkuGAym3JDGxkYN5/xCQt+DuW5GTuRj3yNRpy8uXad+ea0bPjpG4yaF9Q/bTFPPPVOLzxqvn7/h08bXT/8YMpP23RZMaYSdj33PFvoezHUzciIdt3VtkTTNzD5sZqWSbpO0tt85ayV9wWLmSjrO/DVQmJLV0n/2tXfkLzGtv/dafe2GGXolwW1c1NIHkhv1CNs5Fzazr0h6WrHbulY553aZ2T3dxx+RtE6xW7r2KXZb112jfV8A+WtJcKqWrdnZZ+FZVySqO3+0JelzqKUPDC4t92E759YpFsq9H3uk18dO0pfT8V4A8l99XY0eWLtbUmr3YUvU0geGQqUzAGkXr6VfHkitCAq19IGhEdgAMiJeS7+qPDBgq9m4ilKfqsoDalg8l/K8wBDY/ANAxsRr6a9vOqwVjfu0p89uXWO1JDhVC+omMrIGUkBgA8goaukD6UFgA8gaaukDI8ccNgAAHkBgAwDgAQQ2AAAeQGADAOABBDYAAB5AYAMA4AEENgAAHkBgAwDgAQQ2AAAeQGADAOABBDYAAB5AYAMA4AEENgAAHkBgAwDgAQQ2AAAeQGAjY8KRqE6cCikSdbluCgB4nj/XDUBh6QxHtK7pkFY07tfeo23yl5jCUafpEyp1T3Cq6utqVOb35bqZAOA5BDbSZltzq+5ctVmhSFTtXRFJUigSG12/dqRNy9bs1ANrd2v13XM0c3JVLpsKAJ7DJXGkxfbmVt2+cqNaO0I9Yd1fe1dErR0h3bZyo7Y3t2a5hQDgbQQ2Rq0zHNGiVZvVEUoc1P11hGLnd4ZTOx8AQGAjDdY1HVIoEh3Wc0KRqNY3Hc5QiwCg8DCHjVFb0bh/wGXwSePL9ehdc7Tlzfd1+fnjdfj4KX3psZfUGY4Fe3tXRCsa92nhrNpcNBkAPIcRNkYlEnXae7Qt4bEpZ5+hH//hLX3yn57TiVMhLbikps/xPUfbuOULgGdl+9ZVRtgYlfausPwl1rMavLfmYx3afeiEJGnngeOadFZ5n+P+ElN7V1jjxgSy0lYAGK1c3rpKYGNUKkr9Cif57bIrfHpeOxKVxgSsz/Fw1KmilC9BAN6Q61tXuSSOUfGVmKZNqBzRc6dPqJSvxIY+EQByLB9uXSWwMWpLglNVUTq8S0AVpT4tCV6YoRYBQPrky62rBDZGrb6uRgFf3y+llmMduvHB53o+//7zr+vB3+7t+TzgK9GCuolZayMAjFS+3LpKYGPUyvw+rb57jsoDqY2yywOx86kpDsALEt26OpT4ravpRGAjLWZOrlLD4rmqKg8kvTxeUepTVXlADYvnUkscgCcMduvqUNJ96ypLdJE2MydXadPS+VrfdFgrGvdpT59bHsZqSXCqFtRNZGQNwDMGu3V1KOm+dZXARlqV+X1aOKtWC2fVKhJ1au8Kq6LUz2pwAJ402K2rQ0n3ratcEkfG+EpM48YEBg3rbFcKAoDhGOzW1R/deYUmjC1L+tx037rKCBtZl8tKQQAwXEuCU7Vszc4BC8/uenRL0udk4tZVRtj9MOLLrG3Nrbpy+QYtW7NTe460yblYpSDnTlcKunL5BvbLBpA3Et26OpRM3LpKYCs24lvzSos++U/Patqy9br8fz2jC5eu043/9KzWvNLCvs1p0rtS0PiKUj1933UDzsl0paB8xi+LQH7Kl1tXi/6SeK5rwxaLkVYK2rR0fkFfHmd6APCG+K2ri/rlRW8VpT4FfCXUEs+EZLVhH19yVc/HxTziS6dElYJ8JaZvf6ZO//HV6/TY3cBPzfMAABYmSURBVHNU5u/75ZiJSkH5hOkBwFvit64uv6VOM6orZSYFfCYzaUb1WC2/pU6bls7P2OCuaAN7sBHfZ1e8OOCxTNWGLRaJKgUNtV92JioF5Yv+vyxOGl8+YIqAXxaB/BO/dfXpr35C+5bXa+vf3qB9y+v19Fev08JZtRm9IlY0gd1/fnCw2rC7Hrgx4eOFPuLLlGSVgobaL1tKf6WgfJAvGwnEMXcOjEwqt66mU0HPYQ82P3h8kC3SkomP+BbOqs1QiwtTskpBQ+2XLaW/UlA+SPbLor/E9J3PzdRHzxunN95t19d+vk2nQrHz4r8sputrj7lzwHsKdoQ91Pzg4ROdI3rdQhzxZVo+VQrKB8k2Epg6oVL/uvltLfju82rrDOuOuVN6jqVzeoC5c8CbCjKwU9lofKTiIz6kbrBKQUNJd6WgXBtsI4EDrR3a+tYxSdKaVw7oiinj+xwf6S+LvS95p/K9wdw5kJ8Ka+ii4c8PDlchjviyoX+loET7ZfeXiUpBuTbYRgLO9X2s/xnDmR5IdsnbJKWa+cVyax3gFQWXPCPZaLy/wX6eFdqIL1vq62r0wNrdklL/RSoTlYJybbDpgUnjz9BlH6rSy2+36s9mnqctb77f53iqvywOVlug/zuXB3x6+POXqebMMSox0z//5149teNQz/F0z50DGLmCC+xE84NfnnehPnNZrQ61ntL77Z1qOnAi4YhOkqrOCKj1ZFfCY4U44suWeKWg21ZuTOnqR6YqBeVafHpgz5GBl8X3HvlAn71skpbfUqc332vXTza+1ed4Kr8sxi95p3qF6RMzztWRE6d0d3dN5LFlfX8ksNASyB8FN4fdf37wktpx+vTMGv3pQ8/rnp9s1aWTkt/QPmFsmZ5YcpW+//wbCY8X4ogvm+KVgqrKA6ooTRzEFaU+VZUH1LB4bs4ry2XqdqclwakD+t9yrEM3/NNzWvrkTi347vNa8pOXe1aIS6n9sphsOmjcGL/+29zzEz7ntcMf6JoLz9H9N12kK6aM1wedA9dnsNASyA8FNcKOOjdgfnDOlLP09K4j3T/8ovrtq0eSPv/oB536k+88m/BYoY74si1eKWh902GtaNynPX1uKRqrJcGpWlA3MWf/ztm43SlT0wPJpoPGlQd0x9zzB4zYJemNd9v1qX9+QfNmTNBf3XSRnt/7jh7a0Hc1eiHeWgd4UUEFdolZ4vlBN/LRQaZrwxajeKWghbNqFYk6tXeFVVHqz/nagGzVlR/t9EA4EtXJUGTAfHay28X++qaLdP7ZZ2jd/7hGz+99V99e/8eeYxPGlul4R0hPbjugk11h3Xr5pAHPZ6ElkB8K7ruw//zgpjfe1//7uZn6XuN++UtM8z9SrX/d9PaA59WcOUbjxvjzbsRX6OKVggbTO6AyFeqpzP3GwjCi21ZuHPSSfSrtHe5GAhfVjNWaV1oGjPz/ZpbTsVdadOPFE5PeLvYPv/mjplePVf1DLww4dtHEsfpm/UfknFMo4rTsyZ0DzmGhJZAfCi6w+98+tOvgCT2145DW3XutDhzrGLDyVor9YPzrmy7KuxFfMctmJa507CQ2kvamOj3w6qEPdOXyDQlH/qdCES1bs1P/c+1ulZiU4G6xQT239109993nkx5noSWQPwousBPNDz78u316+Hexebn7rp824Dm95wdTGfEhs7K95Wnvud//+7oL1BmO6tEX39Tffuoj+kjNOP3X72/SVVPP1udmT9ZXf7ZNknQqHOm53Wk07R1qemA4I/9MYKElkD8KbpV4vmw0jpHJRSWu3nO/m994X1dMOUuSVFdbpTNK/fKXmK6Ycpa2vHH66sypUFT/+MyeQdt73/XT9KVrL0i5vf03EkhXEaC2zrAqyob/9c33BpBfCi6wpcFvH3rwt3v1/edfz6vbhxCTakBZ90xF/NL0aG446l8qtOnAcdXVnqmKUp9KTDr/rDN06aQzdcWUs7S533TK2++f1B0/SP2e5+HuutV/1felk87U+nuvVZm/ROUBn/7jq9fprDPHDvk6rSdD2vrWMT1933X65oKLhjyf7w0gPxXcJfG4fL99CAMluy1p0vhyPXrXHP1h/3u67PwqLX5sqw60dkiKVeI63hEa9HUHWwTWv1RoOOrU0npSn5s9WU0HjqvmzHLNveBsnX/2GdqXYFHXqdDQRXp6G07lsP6rvne0HNdvXz2ir39yhsYESvTkKwdUfvwDpfJtfG/DtoSPl1isVGnAx/cGkO8KNrCl/L19CIkluy1Jki44p0Lf+MV2/e2v+q5ibu+K6J0PBhb7SHURWKJSoZvfeF9fuu4C/Z/f/FHXTjtXn597vvYfbdOv/8c1+uYTTdrRcrzn3K5ev1/0LtLjLynRU39xzYDATrVyWLJNQh7asFdrv3KNOkMR/c+1u/TVSwZ9mUGVB3xqWDxXl9SeyfcG4AEFeUk8kWxvNI7hGWwXKym2k9UrSeZ/T4UifSpxDWf7yEQ7iW1+431NGFumnQePKxJ1ikadpldX6hu/2NEnrPvrXaSnrTOctEhPKpXD4iP//qrKS3VGqU8VZf6kI2BfienMMf6Uq8nxvQF4Q9EENvJbsoCKOznINqlmp7c8HcmitTuvmtLn+Iv739O0pet1KhTVWRWl6gpHdccPN2v3oRMJX6+PFIr0pLJFa7JNQv6fz9TpO/+xR09uO6j7k8xHR53TH/5mvpbfUqcZ1ZUyi13yNpNmVI/V8lvqtGnpfOanAY8p6Evi8I7BdrEainOxSlwjvZ+68RtB/c2agQVDJOmDUyEdPN6h2VPGD3oFQEq9SE8qlcMSbRLymctiUztrtx9UiUlPLLlKb5w8R2rqe+Vh+oRKnVHqZzoIKDAENvLCYLtYDWVMwCdfiWnt9oGL1j5/5Yf0+Ss/JEkaOyaglmMduv37G3uOhyJRNb72jj50Vrnefr9jwGuHIk6LH9uqx744R+2dEa3dfjBpO1Ip0iOlXjmsfxGgJ14+oCdePiAptlBs4fde1Nfrwur9bZyo0Am1BYDCQGAjb/QPqLiWYx268cHnEj6notSnc8fG5moTLVr76aa39dNNb8tfYvrXL83VD17ou61qfBHYV2+Yrm8+3qRT4YGr1DtCEX3x0S368RevVEcoomd2J99ApneRnmTtTbVyGHuIA+iNOWzkjfq6GgV8w/uSDPhKdGZ5YMhFa9/69MX6w/53teHVowOO7TnaphsvnqgxgYFbXsZ/UThxKqybH/69ntl9RHdeNUW//don9OB/+diw2hpvb6qB6rUiQJnajhRADCNs5I2R7mJ1bP+2AfdT93br5ZNUO75cf7c28Tx1/JavVN/7jrnna9GPNqvl2MBL6Km0dziBOtxNQrK9kCybNd+BYscIG3llsCp1cYkqcSVbtHZJ7Th96doLdN/PXkm6gDu+CCyV916+8BJNPusM/WDRbH3xmg+n1KfRVg6LFwFKtOp7TMCXs1Xfw7l9DsDojWqEbWZnSfqZpCmS3pT0fznnjiU4701JHyg2GRd2zs0ezfuisI2kSl2yRWuLPj5FVWcE1PCluZKkHQeO6/7Hm/qc03sRWPy9f73jkL728+0D2rb0yZ36xIxzdfvKjTp2MnmFNX9JbOesdFUOS1YE6PnnnlUwhapp6ZbO7UgBpGa0l8Tvl7TBOff3ZnZ/9+d/neTcec65d0f5figSI6lSl2jR2jd+uWPQ90m0CKzM79P1H61WwJf4EvtQAj5T4zeCmjiuPCO3UeV61Xc6tiMFMHyjvSR+s6TV3R+vlrRwlK8HDJBqJa6RLlpLtAhsNPeFh6MuY2GdD5LVfB9MvIY6gJEzl0JlpqRPNmt1zlX1+vyYc258gvPekHRMkpP0/znnVg7ymoslLZak6urqyxsaGlJuT1tbmyorK4c+sQDR91jfO0IRvf5Ou6IpfF2XmOmCcyuSrsLee7RtwOYekrTo5hv0s988q1OdXQmfNybgG1DuNFNy8f+e6N/log9P1qyPTJUkvdt6Qs+8+PKA56X734WvefpeiObNm7c12bTxkJfEzey3khLdh7J0GG242jl30MwmSHrGzP7onEt4Y213mK+UpNmzZ7tgMJjymzQ2Nmo45xeSYu/7x6+5tme18p4jg192TXVV9bFXWhLeF35LvfS93T4dOznw26eiNLYILFvzytn+f49Ene5auk7One77tAmVqv/kdN3wzy/q2MmQziwP6HjHwH8bM2nf8k+k7cpDsX/N0/fiM2RgO+euT3bMzI6YWY1z7pCZ1UgaeJNr7DUOdv991MzWSJojKXElDGCYOkIRXbl8Q9LbnuJM0vTq1BeBJStccs0//C7pcwq9cEmi2+euuvAcrW863LMIL9l2p/Ea6lRdA0ZmtHPYayUt6v54kaRf9T/BzCrMbGz8Y0mflJT4hlhgmLY3t+r1d9oH3ewjrsxfov9966VaOKs2pcVPXitckg2J5vZNktPQUxCp1FAHkNxoA/vvJd1gZnsl3dD9uczsPDNb131OtaQXzGy7pM2Sfu2c+80o3xfoWa2cyny1JJ0KR7Vo1WZ1hlMv9TnS+8ILVaLtSH+/7139aV2Nqs6IjZzPLE88gk61hjqAxEb1665z7j1J8xM8flBSfffHr0uaOZr3ARLpvVp50vhyrb5rjra89b5mTR6vVw+d0C+2Nuur10/X2ZVluq/hFW1vOd6zWnnhMOaYR3JfeCHrf/vc3qNtevh3+/SzxR9X1DntOnhcf/mLvrfTDaeGOoDEuD4Fz+q/2cf5Z5+hP//py/rm0Sat/fI1uvljtbr1kT/oho9W68vzLtTiH2/t2exjOIEtjey+8EKVaG7/8ZcP6PHuncQSKfS5fSAbKE0KT0q02UfzsQ69duQDOSftOfqBfr8vVqfnj4dPaNL48p7z9hxtG9UGFaneF16omNsHcoPAhifFVyv31tVra0znXM/nzkm+ktNf6vHVyhg55vaB7OOSODxptJXIWK08esztA9nFTy14UrLNPlLBauX0YW4fyB4uiSPrwpGoTpwKjWoeWYqtVo5fjm051qEbHzxdi+cvf7FD63ceHnCM1cqZU+xz+0CmMcJGVnSGIz2lQ/f2uXRaqXuCU1VfVzPsS6enVyunjtXKALyKETYybltzq65cvkHL1uzUniNtck4KRZyck1470qZla3bqyuUbtL25dVivG1+tXGKpjehYrQzAywhsZNT25lbdvnLjoKVD27siau0I6baVG4cd2jMnV+mCcytYrQyg4BHYyJh46dCOBFtUJtIRigy7dKgUGzlvWjpfy2+p04zqSplJAZ/JTJpRPVbLb6nTpqXzCWsAnsYcNjKmd+lQSbr/povU0tqhn2x8S5J03/XT1NYZ1g+ef6PnnJGUDpVYrQyg8DHCRsb0Lx367zsO6tOX1vR8/qd1NVq341Cf58RLh44Gq5UBFCJG2MiIRKVDdx08obMryzRhbJnOrizV8Y6QDh4/NeC58dKhBC4AnEZgIyPipUNDkb73Wq9rOqT6uhqdO7ZM/95vdB0XLx06bkzibRoBoBhxSRwZkax06L9vP6hPzzxPCy6ZqHVNiQOb0qEAMBCBjYyIlw7tb+/RNlWU+XTkRKfe+aAz4XMpHQoAAxHYyJjepUN7u+nB53X79zcmfA6lQwEgMQIbGVNfV6OAb3hfYpQOBYDECGxkTLx0aHkgtVKglA4FgOQIbGTUzMlValg8l9KhADBKLMVFxs2cXKVNS+drfdNhrWjcpz19dusaqyXBqVpQN5GRNQAMgsBGVlA6FABGh8BG1sVLhwIAUsccNgAAHkBgAwDgAQQ2AAAeQGADAOABBDYAAB5AYAMA4AEENgAAHkBgAwDgAQQ2AAAeQGADAOAB5pzLdRuSMrN3JL01jKecI+ndDDUn39H34kTfixN9L1znO+fOTXQgrwN7uMzsJefc7Fy3IxfoO30vNvSdvhcbLokDAOABBDYAAB5QaIG9MtcNyCH6Xpzoe3Gi70WooOawAQAoVIU2wgYAoCB5OrDN7Cwze8bM9nb/PT7JeVVm9ksz+6OZvWpmH892W9Mt1b53n+szs1fM7KlstjFTUum7mU02s991/3/vMrN7c9HWdDGzm8zsNTPbZ2b3JzhuZvZQ9/EdZnZZLtqZbin0+/Pd/d1hZi+a2cxctDMThup7r/OuMLOImd2azfZlUip9N7OgmW3r/v5+NtttzAVPB7ak+yVtcM5Nk7Sh+/NEvivpN865iyTNlPRqltqXSan2XZLuVWH0OS6Vvoclfd059xFJcyV92cw+msU2po2Z+SQ9LGmBpI9Kuj1BXxZImtb9Z7GkFVltZAak2O83JH3COXeppP+lApnfTLHv8fP+QdLT2W1h5qTSdzOrkvQ9SX/mnLtY0uey3tAc8Hpg3yxpdffHqyUt7H+CmY2TdJ2kH0qSc67LOdeatRZmzpB9lyQzmyTpTyX9IEvtyoYh++6cO+Sce7n74w8U+4WlNmstTK85kvY55153znVJalDs36C3myU95mI2Sqoys5psNzTNhuy3c+5F59yx7k83SpqU5TZmSir/55L0F5Iel3Q0m43LsFT6/l8lPeGce1uSnHOF1P+kvB7Y1c65Q1LsB7SkCQnOuUDSO5J+1H1Z+AdmVpHNRmZIKn2XpAcl/ZWkaLYalgWp9l2SZGZTJM2StCnjLcuMWknNvT5v0cBfPlI5x2uG26cvSlqf0RZlz5B9N7NaSbdIeiSL7cqGVP7fp0sab2aNZrbVzL6QtdblkD/XDRiKmf1W0sQEh5am+BJ+SZdJ+gvn3CYz+65il1D/Nk1NzJjR9t3MPiXpqHNuq5kF09m2TEvD/3v8dSoVG4Hc55w7kY625YAleKz/7R2pnOM1KffJzOYpFtjXZLRF2ZNK3x+U9NfOuYhZotM9K5W++yVdLmm+pHJJfzCzjc65PZluXC7lfWA7565PdszMjphZjXPuUPflv0SXRVoktTjn4qOrX2rw+d68kYa+Xy3pz8ysXtIYSePM7CfOuf+WoSanTRr6LjMLKBbWP3XOPZGhpmZDi6TJvT6fJOngCM7xmpT6ZGaXKjbls8A5916W2pZpqfR9tqSG7rA+R1K9mYWdc09mp4kZk+rX+7vOuXZJ7Wb2nGLrkwo6sL1+SXytpEXdHy+S9Kv+JzjnDktqNrMZ3Q/Nl7Q7O83LqFT6/k3n3CTn3BRJt0n6Ty+EdQqG7LvFfor9UNKrzrl/zGLbMmGLpGlm9mEzK1Xs/3Jtv3PWSvpC92rxuZKOx6cNPGzIfpvZhyQ9IemOAhtdDdl359yHnXNTur+/fynpzwsgrKXUvt5/JelaM/Ob2RmSrlRhLaxNzDnn2T+SzlZslfDe7r/P6n78PEnrep33MUkvSdoh6UlJ43Pd9mz1vdf5QUlP5brd2eq7YpdGXff/+bbuP/W5bvso+lyv2Ohhv6Sl3Y/dI+me7o9NsZW1+yU1SZqd6zZnqd8/kHSs1//xS7luc7b63u/cRyXdmus2Z7Pvkr6h2OBrp2JTXjlvd6b/UOkMAAAP8PolcQAAigKBDQCABxDYAAB4AIENAIAHENgAAHgAgQ0AgAcQ2AAAeACBDQCAB/z/EMmieyje1UoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize dimensions 0 and 1 of the embedding matrix C for all characters\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = [0] * block_size\n",
    "C[torch.tensor([context])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caraah.\n",
      "ami.\n",
      "hevitkimlex.\n",
      "taty.\n",
      "sacanan.\n",
      "jazonte.\n",
      "dperahci.\n",
      "aqaic.\n",
      "kamaratceriiv.\n",
      "kaleigph.\n",
      "ama.\n",
      "kin.\n",
      "qainn.\n",
      "sroilea.\n",
      "vambi.\n",
      "watelo.\n",
      "diaryxi.\n",
      "chaenniusa.\n",
      "mem.\n",
      "edi.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "      logits = h @ W2 + b2\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
